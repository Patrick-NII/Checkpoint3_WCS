# -*- coding: utf-8 -*-
"""Data_Checkpoint_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19AEldJ6fUKZKRB5QFKelhVYAVqBwdIlf
"""


"""The durations are indicative and obviously depend on each person. If you see that you can't finish an exercise in time, don't hesitate to move to the next exercise.

If you don't have time to finish everything, you can start again this weekend or next week. You will simply indicate in the comments the scripts you have completed afterwards.
"""



"""# Part 1 - API - around 1h

The following dataset lists a selection of the best restaurants in Paris, at very affordable prices (less than 15 euros per menu on average).
You will use this  [API](https://adresse.data.gouv.fr/api-doc/adresse), to retrieve coordinate points (lon, lat) for each restaurants.


"""

import pandas as pd
import requests
import numpy as np
import json
import folium
from skimpy import skim
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

food_paris = pd.read_csv("https://raw.githubusercontent.com/WildCodeSchool/wilddata/main/food.csv").drop(columns = "Unnamed: 0")

food_paris.head()

def get_cordinates(adress):
  url =f"https://api-adresse.data.gouv.fr/search/?q={adress}"
  response = requests.get(url)
  if response.status_code == 200:
    data = response.json()
    return data["features"][0]["geometry"]["coordinates"]
  else:
    return np.nan
food_paris["coordinates"] = food_paris["adresse"].apply(get_cordinates)


def switch_coord(coordinates):
  return (coordinates[1], coordinates[0])
food_paris["coordinates"] = food_paris["coordinates"].apply(switch_coord)
food_paris

"""# Folium

You will now project these restaurants on a map, using the `Folium` library.
"""

m = folium.Map(location=[48.8566, 2.3522], zoom_start=12)


for inde, row in food_paris.iterrows():
  folium.Marker(location=row["coordinates"], popup=row["nom"]).add_to(m)

m

"""# Sorting and grouping

You can now build a descending ranking,  considering the number of restaurants per district.
"""

rastaurant_nb = food_paris['code postal'].value_counts().reset_index()
rastaurant_nb.columns = ['code postal', 'nb_restaurants']
rastaurant_nb = rastaurant_nb.sort_values(by='nb_restaurants', ascending=False)
rastaurant_nb

"""# Part 2 - Data exploration & processing - Python & Pandas - around 30 min

Run the code below. You will get a DataFrame with 10 000 restaurant comments.
- date: the date of the comment
- stars: the rating (from 1 to 5)
- text: the text of the review
- useful: the number of users who indicated this comment as "useful"
- sentiment: the word "good" or "bad" depending on the stars rating

The goal of these parts will be to explore this dataset, then to propose a prediction model of positive rating according to the other features.

Start by checking that there are no missing values.
"""

df_restaurants = pd.read_csv("https://github.com/WildCodeSchool/wilddata/raw/main/restaurant.zip", index_col='Unnamed: 0').loc[:,["date", "stars", "text", "useful"]]
df_restaurants["sentiment"] = df_restaurants["stars"].apply(lambda x: "bad" if x <=3 else "good")
df_restaurants

skim(df_restaurants)

df_restaurants.duplicated(keep=False).sum()

duplicate = df_restaurants[df_restaurants.duplicated(keep=False)]
duplicate

df_restaurants = df_restaurants.drop_duplicates()

df_restaurants.duplicated(keep=False).sum()

date_to_datetime = lambda x: pd.to_datetime(x)
df_restaurants["date"] = df_restaurants["date"].apply(date_to_datetime)

df_restaurants.info()

"""## Column "text"

Use **apply** to create a new column `len_text`, indicating the length (number of characters) of each comment.

Draw a histogram and a boxplot to represent the distribution of this "len_text" column. Are the values well distributed? Are most comments long or short?
"""

df_restaurants.loc[:,"len_text"] = df_restaurants["text"].apply(len)
df_restaurants

plt.figure(figsize=(16,5))

plt.subplot(1,2,1)
sns.boxplot(x="len_text", data=df_restaurants, showfliers=False)
plt.title('Distribution de la longueur des commentaires')
plt.xlabel('Longueur des commentaires')
plt.ylabel('Fréquence')

plt.subplot(1,2,2)
sns.histplot( x="len_text", data=df_restaurants, bins=50)
plt.title('Distribution de la longueur des commentaires')
plt.xlabel('Longueur des commentaires')
plt.ylabel('Fréquence')


plt.show()
df_restaurants['len_text'].describe()
#la plupart des commentaires sont plutot long et plutot réparties entre le 50éme et 75éme percentile

"""## Column "stars"

- Draw a scatterplot to compare the `useful` column and the `stars` column. Do you think there is a correlation? Please compute the correlation coefficient.

- Ditto between `len_text` and `useful`.

- Please comment and interpret the results.


"""

plt.figure(figsize=(16,5))

plt.subplot(1,2,1)
sns.scatterplot(x="useful", y="stars", data=df_restaurants, hue='useful')
plt.title('Correlation entre la note du commentaire et la note globale')
plt.xlabel('useful')
plt.ylabel('stars')

plt.subplot(1,2,2)
sns.scatterplot(x="len_text", y="useful", data=df_restaurants, hue='useful')
plt.title('Correlation entre la longueur des commentaires et sa note')
plt.xlabel('len_text')
plt.ylabel('useful')

plt.show()

# la colonne 'useful' je l'ai compris comme une note du commentaire, en notaznt celui ci comme pertinent ou pas, ci cela a etais utile ou pas

def get_correlation(column1, column2=df_restaurants['useful']):
  correlation_result = round(column1.corr(column2), 3)
  if 0 < correlation_result <= 0.399:
    return f"La correlation entre {column1.name} et {column2.name} est de {correlation_result}, il y a une faible correlation entre ces deux variables"
  elif 0.4 < correlation_result <= 0.5:
    return f"La correlation entre {column1.name} et {column2.name} est de {correlation_result}, il y a une moyenne correlation entre ces deux variables"
  elif 0.6 < correlation_result <= 0.799:
    return f"La correlation entre {column1.name} et {column2.name} est de {correlation_result}, il y a une forte correlation entre ces deux variables"
  elif 0.8 < correlation_result <= 1:
    return f"La correlation entre {column1.name} et {column2.name} est de {correlation_result}, il y a une très forte correlation entre ces deux variables"
  else:
    return f"La correlation entre {column1.name} et {column2.name} est de {correlation_result}, il y a une très faible correlation entre ces deux variables"


print(get_correlation(df_restaurants['len_text']))
print(get_correlation(df_restaurants['stars']))

"""# Part 3 : NLP - Sentiment analysis classification - 2h

Define `X` which will contain only the `text` column. And `y` will be the `sentiment` column.
"""

X = df_restaurants['text']
y = df_restaurants['sentiment']

"""## Create a function to clean up stopwords and punctuation

You can call your function `func_clean`.
Your function must take a `str` as a single parameter, and return a `str`.

For example:

`func_clean("Hello, how are you? Fine, thank you.")`

`>>> 'hello fine thank'`
"""

def func_clean(text) -> str:
  text = text.lower()
  text = re.sub(r'[^\w\s]', '', text)
  stop_words = set(stopwords.words('english'))
  words = word_tokenize(text)
  filtered_words = [word for word in words if word not in stop_words]
  return ' '.join(filtered_words)

func_clean("Hello, how are you? Fine, thank you.")

"""## Apply this function

Apply this function to `X` and store the result in `X_clean`.
"""

X_clean = X.apply(func_clean)

"""## Train test split

Split your `X_clean` and `y` data with a train test split, and the same `random_state = 32`.


"""

X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=32)

"""## TfidfVectorizer

- Train `TfidfVectorizer` on `X_train`.
- Transform `X_train` with your vectorizer and store the result in `X_train_vecto`.
- Transform `X_test` with your vectorizer and store the result in `X_test_vecto`.
"""

tfidf = TfidfVectorizer()
X_train_vecto = tfidf.fit_transform(X_train)
X_test_vecto = tfidf.transform(X_test)

"""## Logistic regression

Train a logistic regression on `X_train_vecto` and `y_train`.

Please compare the accuracy scores of the training and test sets. Is there any overfitting?

Also display a confusion matrix for the test set. How many "bad" comments are correctly predicted?
"""

model_LG = LogisticRegression().fit(X_train_vecto, y_train)
train_score = round(model_LG.score(X_train_vecto, y_train), 3)
test_score = round(model_LG.score(X_test_vecto, y_test), 3)
print(f"Le score de l'entrainement est de {train_score}")
print(f"Le score du test est de {test_score}")
if train_score > test_score:
  print("Il y a un risque d'overfitting")

y_pred = model_LG.predict(X_test_vecto)

matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'])
plt.show()

"""## Decision tree
Train a decision tree on `X_train_vecto` and `y_train`.

Please compare the accuracy scores of the training and test sets. Is there any overfitting? Are the scores better than previously?
"""

model_DT = DecisionTreeClassifier().fit(X_train_vecto, y_train)
train_score_DT = round(model_DT.score(X_train_vecto, y_train), 3)
test_score_DT = round(model_DT.score(X_test_vecto, y_test), 3)
print(f"Le score de l'entrainement est de {train_score_DT}")
print(f"Le score du test est de {test_score_DT}")
if train_score_DT > test_score_DT:
  print("Il y a un risque d'overfitting")

  "le scores d'entrainement est parfait mais le score des test est beaucoup plus faible ce qui nous crée de l'overfitting"

y_pred2 = model_DT.predict(X_test_vecto)

matrix2 = confusion_matrix(y_test, y_pred2)
sns.heatmap(matrix2, annot=True, fmt='d', cmap='Blues', xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'])
plt.show()

"""## Bonus question (optional)

Use `GridsearchCV` to find the best parameters.
Make sure you don't do more than 1000 different iterations (i.e. combinations).
Otherwise, the time may seem very long...




"""

model_RF = RandomForestClassifier()

param_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}


grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid,
                           cv=3, n_jobs=-1, verbose=2)


grid_search.fit(X_train_vecto, y_train)

# Best parameters
best_params = grid_search.best_params_
print(f"Meilleurs paramètres : {best_params}")

best_model = grid_search.best_estimator_
train_score = best_model.score(X_train_vecto, y_train)
test_score = best_model.score(X_test_vecto, y_test)
print(f"Score d'entraînement : {train_score}")
print(f"Score de test : {test_score}")

y_pred3 = best_model.predict(X_test_vecto)

matrix3 = confusion_matrix(y_test, y_pred3)

plt.figure(figsize=(8, 6))
sns.heatmap(matrix3, annot=True, fmt='d', cmap='Blues', xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'])
plt.show()

"""# Optionnel : Algorithme `JSON` et manipulation.
This is a json file containing several keys.
Each key has a value, which could potentially be another key, containing another value, which could potentially be another key etc...
"""

food = {
  "clé1": {
    "fruit1": "pomme",
    "légume4": "brocoli"
  },
  "clé2": {
    "légume1": "carotte",
    "fruit5": "banane",
    "légume3": "courgette"
  },
  "clé3": {
    "niveau1": {
      "niveau2": {
        "fruit3": "orange",
        "légume5": "aubergine",
        "fruit5": "mangue"
      }
    }
  },
  "clé4": {
    "niveau1": {
      "niveau2": {
        "niveau3": {
          "fruit6": "raisin",
          "fruit7": "fraise",
          "légume4": "poivron",
          "fruit2": "pastèque"
        }
      }
    }
  }
}

"""Problem:
Fruits and vegetables have been misplaced in this json file. The goal is to retrieve each of the fruits and vegetables, and assign them to two corresponding lists: `fruits_list` & `vegetables_list`.

Expected solution:

`fruits_list` = `['pomme', 'banane', 'orange', 'mangue', 'raisin', 'fraise', 'pastèque']`

`vegetables_list` = `['brocoli', 'carotte', 'courgette', 'aubergine', 'poivron']`


"""

fruits_list = ['pomme', 'banane', 'orange', 'mangue', 'raisin', 'fraise', 'pastèque']
vegetables_list = ['brocoli', 'carotte', 'courgette', 'aubergine', 'poivron']

fruits_list_2 = []
vegetables_list_2 = []

def  get_fruits_vegetables(food):
  if isinstance(food, dict):
    for k, v in food.items():
      get_fruits_vegetables(v)
  elif isinstance(food, list):
    for item in food:
      get_fruits_vegetables(item)
  else:
    if food in fruits_list:
      fruits_list_2.append(food)
    elif food in vegetables_list:
      vegetables_list_2.append(food)

get_fruits_vegetables(food)
print(fruits_list_2)
print(vegetables_list_2)

"""Next, you'll create a new dictionary, which will simply contain two keys: `fruits` & `vegetables`. Each key will have the value of the list of fruits and the list of vegetables. That way, everything will be in order.

Expected solution:
`food_dict` = `{'fruits': ['pomme',
  'banane',
  'orange',
  'mangue',
  'raisin',
  'fraise',
  'pastèque'],
 'legumes': ['brocoli', 'carotte', 'courgette', 'aubergine', 'poivron']}`
"""

fruits_dict = {
    'fruits': fruits_list_2,
    'vegetables': vegetables_list_2
}
print(fruits_dict)